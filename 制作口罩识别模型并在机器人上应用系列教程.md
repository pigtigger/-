## 制作口罩识别模型并在机器人上应用系列教程

##### 建议了解

1. 目标检测原理以及相关应用
2. 常用轻量型目标检测模型及相关文献阅读
3. 口罩识别相关数据集及如何标注
4. 如何训练模型并进行测试
5. 如何将模型部署到嵌入式设备

##### 前言

​		第一部分及第二部分建议大家自己去了解，接下来我们将3,、4、5分别做一个详细的介绍。

​		这里使用的系统是Windows ，建议在进行下列步骤之前先安装Anaconda3，直接登录[官网](https://www.anaconda.com/download/)下载安装即可，Anaconda主要方便于建立虚拟环境以及各种库的管理。

### Part1 口罩识别相关数据集及如何标注

​		要对戴口罩以及不戴口罩的人群进行加以区分，那么需要戴口罩以及不戴口罩相关人脸照片，不戴口罩的人脸图片较容易找到，因为是简单的人脸信息，那么戴口罩的数据集我们需要从哪获取呢？这里有几种方式，第一种就是通过摄像头一个个截取你与你的伙伴不同角度的人脸图片，这样子会比较费时费力，不大推荐；第二种便是在互联网上寻找疫情期间相关的采访视频，截取出现戴口罩的图像作为接下来的标注素材，这种方式也是可取的，可以采集到更加贴切真实场景的图片素材；第三种便是借助相关数据集，这里推荐使用的是国内的一家面部遮挡数据集。

​		MAFA是一个面部遮挡的面部检测数据集，包含30,811张普通人脸图像和35,806 张面部遮挡图片。数据集内的人脸图像具有各种不同的方向和遮挡度，而每个人脸的至少一部分被遮罩遮挡。

![image-20200928225917674](C:\Users\TomTom\AppData\Roaming\Typora\typora-user-images\image-20200928225917674.png)

##### 1.1 labelImg安装

​		这里需要用到常用的图片标注工具labelImg，可用于数据集的标注及制作等。安装方式比较简单。在Anaconda Prompt或者命令提示符中依次输入以下命令。

```
pip install PyQt5 -i https://pypi.tuna.tsinghua.edu.cn/simple/（后面这行是国内的清华镜像源，下载速度才会比较快）
```

```
pip install pyqt5-tools -i https://pypi.tuna.tsinghua.edu.cn/simple/
```

```
pip install lxml -i https://pypi.tuna.tsinghua.edu.cn/simple/
```

```
pip install labelimg -i https://pypi.tuna.tsinghua.edu.cn/simple/ 
```

全部安装完毕后直接在Anaconda Prompt或者命令提示符中打开labelImg，打开范围为直接输入，便出现以下工作界面。

```
labelimg
```

<img src="C:\Users\TomTom\AppData\Roaming\Typora\typora-user-images\image-20200930085242857.png" alt="image-20200930085242857" style="zoom: 50%;" />

<img src="C:\Users\TomTom\AppData\Roaming\Typora\typora-user-images\image-20200930085554508.png" alt="image-20200930085554508" style="zoom: 33%;" />

##### 1.2 数据集标注方法

​		labelimg使用起来比较容易，标注方法为使用矩形标注你认为感兴趣的区域，如在这次口罩识别任务中感兴趣的区域为戴口罩以及不带戴口罩的人脸部分，那么只要如下图这么标注即可，戴口罩这里我标注为face-mask，不戴口罩标注为un-mask，当然这些标签可以由你自定义。

<img src="C:\Users\TomTom\AppData\Roaming\Typora\typora-user-images\image-20201001102741265.png" alt="image-20201001102741265" style="zoom: 33%;" />

​		小技巧以及主要快捷键：

​        左上角需要Open Dir打开你存放图片素材的文件夹，另外建立一个文件夹用于存放标签文件，用Change Save Dir打开，接下来的xml格式标签文件将保存至该文件夹中。

​		快捷键主要用到的是W,A,D

```
  绘制矩形框 W                        上一张图片 A                       下一张图片 D
```

### PART2 如何训练模型并进行测试

​		当前适用于嵌入式设备以及边缘计算的目标检测模型主要有MobileNet、YOLO、ShuffleNet系列等。主流的深度学习框架有Tensorflow、Pytorch等，在这里我们以Pytorch框架下、使用2020年推出轻量级YOLOv4tiny为例训练口罩识别模型进行介绍，当然同学们可以选择其他类型的模型以及框架进行训练，不影响后续深度学习模型在机器人等嵌入式设备上的部署。训练部分可以在自己的笔记本电脑以及云服务器完成。

​       Tensorflow模型训练教程建议参考https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10。接下来便是详细的训练部分教程。		

##### **2.1 文件准备**

​		YOLO v4-Tiny模型可从GitHub上获得。首先在任意位置新建一个自己的项目文件夹用于存放代码文件（注意文件夹名字及路径中不能有有**中文或空格**，否则之后会出现各种问题），打开命令行cd到此文件夹下，运行

```
git clone https://github.com/xxlbigbrother/Make-a-mask-recognition-model-and-apply-it-on-the-robot.git
```

​		也可以直接从GitHub页面上下载压缩包，

​		下载得到的YOLO v4-Tiny文件夹架构如下：

<img src="file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps1.jpg" alt="img" style="zoom: 50%;" />

​		为了简化模型的训练过程，通常我们都会利用已预训练过的模型权重进行迁移学习，也就是将预训练好的模型权重参数作为自己模型的初始权重参数进行训练。因此，除了代码，还需要下载预训练好的模型权重文件。

​		可以从百度网盘上下载权重文件，有两个文件可供选择，分别是利用voc数据集预训练的yolov4_tiny_voc.pth和利用coco数据集预训练的yolov4_tiny_weights_coco.pth，下载好后需要将权重文件放入model_data文件夹下。

​		训练所需的yolov4_tiny_weights_coco.pth和yolov4_tiny_weights_voc.pth可在百度网盘中下载。

​		链接: https://pan.baidu.com/s/1xtVnJUwt97vqrzrLTsScOA 提取码: rv24



***\*预训练模型测试\****

1、Python运行predict.py示例程序，输入img/street.jpg，可以看到打开了img文件夹下名为street.jpg的图片，图中各种物体已经被框出并分类。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps2.jpg)

如出现FileNotFoundError错误，请学习python工作目录、相对路径等知识，有能力的同学可使用os.path模块改进代码，一劳永逸地解决相关问题。



2、Python运行video.py示例程序可以打开摄像头进行检测。



***\*模型训练\****

***\*一、数据准备\****

训练模型需要大量数据，有能力的同学可以自行在网上寻找高质量的数据集使用，这里我提供一些戴口罩与未带口罩的人脸图片，供同学们下载选用。

链接：[https://pan.baidu.com/s/](https://pan.baidu.com/s/1xtVnJUwt97vqrzrLTsScOA) 提取码: 。。。。



图片下载下来并挑选后，需要放到/VOCdevkit/VOC2007/JPEGImages文件夹下。注意，这些图片的文件名是混乱的，为了方便后续的标注和训练，应当将它们的文件名变成规律的，如“0000.jpg”、“0001.jpg”等。实现方法请同学们自行探索。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps3.jpg)

此外，有能力的同学可以利用数据增强（Data Augmentation）来扩充数据集，提高模型的鲁棒性。



***\*二、数据标注\****

接下来进行数据标注，需要使用标注软件，这里用的是labelImg工具，可以从[此处](https://github.com/tzutalin/labelImg)下载并学习安装方法。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps4.jpg)

运行labelImg，打开界面

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps5.jpg)

点击界面左上的Open Dir选择图片文件夹，即/VOCdevkit/VOC2007/JPEGImages，之后点击Change Save Dir选择标签存储文件夹为/VOCdevkit/VOC2007/Annotations，标注格式选择PascalVOC。



点击Create RectBox就可以进行框选标注，框选带口罩或不戴口罩的人脸，在弹出的标签窗口中输入mask或un_mask即可进行标注，之后点Save保存，Annotations文件夹下就生成了xml格式的标签文件，之后点击Next Image继续进行标注（活用快捷键W、A、D、Ctrl+S）。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps6.jpg)

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps7.jpg)

标注完成后，Annotations文件夹下会生成大量的xml文件

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps8.jpg)

打开任意xml文件，格式如下

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps9.jpg)

但此时标注文件尚不能被YOLO模型读取，要运行/VOCdevkit/VOC2007文件夹下的voc2yolo4.py提取文件索引，之后运行根目录下的voc_annotation.py，运行前需要将classes改成自己需要的classes，即mask和un_mask。运行后索引和标注信息被写入到了根目录下的2007_train.txt文件中。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps10.jpg)



***\*三、训练模型\****

接下来进入模型训练的环节，首先要在model_data下新建一个txt文档，文档中输入需要分的类，同时在train.py中将classes_path指向该文件。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps11.jpg)

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps12.jpg)

之后，在train.py中设置超参数，如学习率lr、批量大小Batch_size、训练世代数等，可以先使用默认的值进行实验，之后再根据情况进行修改。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps13.jpg)

注意，代码中利用torch.save模块将训练出的模型权重文件保存在根目录的logs文件夹下，保存频率为每世代保存一次，同学们可以根据自己的意愿修改保存频率及路径。



一切就绪后，运行train.py开始训练，注意观察loss的下降。训练结束后logs文件夹下得到诸多权重文件。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps14.jpg)

找到最后世代的权重文件，将根目录下yolo.py中的model_path指向该权重文件，并将classes_path指向之前在model_data下创建的txt文件。

![img](file:///C:\Users\TomTom\AppData\Local\Temp\ksohtml20556\wps15.jpg)

接下来同测试预训练模型时一样，运行predict.py和video.py进行模型效果的测试。如测试结果不够理想，可以尝试修改超参数、扩大数据集等方法。

 



